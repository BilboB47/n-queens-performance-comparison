# N-Queens Solver: CPU vs GPU Comparison

## Overview

This project focuses on **performance comparison of three different algorithms** for solving the classical **N-Queens problem**, implemented using both CPU and GPU approaches.

The main goal is to analyze how algorithmic optimizations and parallel computing (CUDA) impact execution time as the problem size grows.

The project is intended for **educational and portfolio purposes**, with an emphasis on clean architecture, reproducible benchmarks, and clear visualization of results.

---

## Implemented Algorithms

The following three approaches are compared:

1. **Naive Backtracking (C++)**  
   - Classic recursive solution  
   - Easy to understand, but highly inefficient for large `N`

2. **Bitmask Optimization (C++)**  
   - Uses bitwise operations to track columns and diagonals  
   - Significantly reduces computation time on CPU

3. **CUDA Acceleration (CUDA / C++)**  
   - Parallel exploration of the search space on GPU  
   - Demonstrates the benefits of massive parallelism

---

## Project Structure

```text
.
├── src/                    # Source files (.cpp, .cu)
│   ├── queens_naive.cpp
│   ├── queens_bitmask.cpp
│   └── queens_cuda.cu
├── tests/                  # Python unit tests validating correctness
│   └── unittest_nqueens.py
├── benchmarks/             # Benchmarking and plotting scripts
│   ├── run_benchmarks.sh
│   ├── plot_results.py
│   ├── nqueens_results.csv # Generated data
│   └── performance_chart.png # Generated chart
├── build/                  # Build directory (generated by CMake)
├── CMakeLists.txt          # CMake configuration
├── .gitignore              # Git ignore rules
└── README.md               # Project documentation
```
---

## Requirements

To build and run the project, the following tools are required:

- **CMake** (>= 3.10)
- **C++ compiler** with C++17 support
- **CUDA Toolkit** (for GPU implementation)
- **Python 3**
  - `pandas`
  - `matplotlib`

Install Python dependencies with:

```bash
pip install pandas matplotlib
```

---

## Build Instructions

The project uses **CMake** as its build system.

From the root directory, run:

```bash
mkdir build
cd build
cmake ..
make
```

All compiled binaries will be generated inside the build/ directory.

---

## Running Unit Tests

Unit tests are implemented in Python and are used to verify the correctness of all N-Queens implementations.

To run the tests, execute the following command from the root directory:

```bash
python3 tests/unittest_nqueens.py
```

---

## Running Benchmarks

Benchmarking is performed in two steps: generating performance data and visualizing the results.

## Step 1: Generate Benchmark Data

Navigate to the `benchmarks` directory and run:

```bash
cd benchmarks
chmod +x run_benchmarks.sh
./run_benchmarks.sh
```

This script executes all implementations for selected values of N and stores execution times in a CSV file.

---

## Step 2: Generate Performance Plot

After the CSV file is generated, create the performance comparison plot:

```bash
python3 plot_results.py
```

The resulting plot is saved as a PNG file.

---

## Benchmark Visualization

The benchmark results are visualized in a single comparison plot.

- The Y-axis uses a **logarithmic scale** to clearly show differences in computational complexity.
- Each curve represents a different implementation:
  - Naive Backtracking (CPU),
  - Bitmask Optimization (CPU),
  - CUDA Acceleration (GPU).
- The plot highlights how algorithmic optimizations and parallel execution affect scalability as `N` increases.

### Example Benchmark Plot

![Benchmark Results](benchmarks/performance_chart.png)

> The image shows execution time versus board size (`N ≤ 16`) on a logarithmic scale.

---

## Notes

- GPU benchmarks require a **CUDA-capable NVIDIA GPU**.
- Performance results may vary depending on hardware configuration, CUDA version, and system load.
- Small values of `N` may not benefit from GPU acceleration due to kernel launch overhead.
- The primary goal of this project is **relative performance comparison**, not absolute timing accuracy.
















